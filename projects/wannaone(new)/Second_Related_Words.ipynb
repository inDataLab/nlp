{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_170811_wannaone_bae_jinyoung_Woo_merged\n",
      "predicted_170811_wannaone_ha_sungwoon_Woo_merged\n",
      "predicted_170811_wannaone_hwang_minhyun_Woo_merged\n",
      "predicted_170811_wannaone_kang_daniel_Woo_merged\n",
      "predicted_170811_wannaone_kim_jaehwan_Woo_merged\n",
      "predicted_170811_wannaone_lai_kuanlin_Woo_merged\n",
      "predicted_170811_wannaone_lee_daehwi_Woo_merged\n",
      "predicted_170811_wannaone_ong_sungwoo_Woo_merged\n",
      "predicted_170811_wannaone_park_jihoon_Woo_merged\n",
      "predicted_170811_wannaone_park_woojin_Woo_merged\n",
      "predicted_170811_wannaone_yoon_jisung_Woo_merged\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# 특정 디렉토리 아래의 모든 파일의 절대경로 목록을 리턴하는 함수\n",
    "def allfiles(path):\n",
    "    res = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        rootpath = os.path.join(os.path.abspath(path), root)\n",
    "\n",
    "        for file in files:\n",
    "            filepath = os.path.join(rootpath, file)\n",
    "            if 'xlsx' in filepath:\n",
    "                res.append(filepath)\n",
    "    return res\n",
    "\n",
    "import re\n",
    "def ex_keyword(h_x,keyword):\n",
    "    for i in range(len(h_x)):\n",
    "        try:\n",
    "            h_x[i] = h_x[i].replace(keyword,\"\")\n",
    "        except:\n",
    "            h_x[i] = str(h_x[i])\n",
    "            pass\n",
    "    return h_x\n",
    "\n",
    "# freq_1: 첫번째\n",
    "def freq_1(doc_all):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from konlpy.tag import Twitter\n",
    "    twit = Twitter()\n",
    "    def tokenizer2 (doc):\n",
    "        tagged = ['/'.join(t) for t in twit.pos(doc)]\n",
    "        result_words = [word for word in tagged if word.split('/')[-1] in ['Noun','Adjective']]\n",
    "        result_nouns = [word.split('/')[0] for word in result_words if len(word.split('/')[0]) > 1]\n",
    "        return result_nouns\n",
    "    count_vect = CountVectorizer(tokenizer=tokenizer2,min_df=0)\n",
    "    count_all = count_vect.fit_transform(doc_all)\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    tf_transformer = TfidfTransformer().fit(count_all)\n",
    "    tf_all = tf_transformer.transform(count_all)\n",
    "    tf_all_array = tf_all.toarray()\n",
    "    sort_index = tf_all_array.argsort(axis=1)\n",
    "    count_dict = count_vect.vocabulary_\n",
    "    subset_a = []\n",
    "    for index in sort_index[0][-20:]:\n",
    "        important_word = index\n",
    "        key = list(count_dict.keys())[list(count_dict.values()).index(important_word)]\n",
    "        subset_a.append(key)\n",
    "    subset_b = []\n",
    "    for index in sort_index[1][-20:]:\n",
    "        important_word = index\n",
    "        key = list(count_dict.keys())[list(count_dict.values()).index(important_word)]\n",
    "        subset_b.append(key)\n",
    "    subset_c = []\n",
    "    for index in sort_index[2][-20:]:\n",
    "        important_word = index\n",
    "        key = list(count_dict.keys())[list(count_dict.values()).index(important_word)]\n",
    "        subset_c.append(key)\n",
    "    result = [subset_a,subset_b,subset_c]\n",
    "    return result\n",
    "\n",
    "# freq: 두번째\n",
    "def freq(level, subset):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from konlpy.tag import Twitter\n",
    "    twit = Twitter()\n",
    "    def tokenizer2 (doc):\n",
    "        tagged = ['/'.join(t) for t in twit.pos(doc)]\n",
    "        result_words = [word for word in tagged if word.split('/')[-1] in ['Noun','Adjective']]\n",
    "        result_nouns = [word.split('/')[0] for word in result_words if len(word.split('/')[0]) > 1]\n",
    "        return result_nouns\n",
    "    count_vect = CountVectorizer(tokenizer=tokenizer2,min_df=0)\n",
    "    count_all = count_vect.fit_transform(level)\n",
    "    count_sub = count_vect.transform(subset)\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    tf_transformer = TfidfTransformer().fit(count_all)\n",
    "    tf_all = tf_transformer.transform(count_sub)\n",
    "    tf_all_array = tf_all.toarray()\n",
    "    sort_index = tf_all_array.argsort(axis=1)\n",
    "    count_dict = count_vect.vocabulary_\n",
    "    subset_a = []\n",
    "    for index in sort_index[0][-20:]:\n",
    "        important_word = index\n",
    "        key = list(count_dict.keys())[list(count_dict.values()).index(important_word)]\n",
    "        subset_a.append(key)\n",
    "    return subset_a\n",
    "\n",
    "def main(doc_all,level_all):\n",
    "    step_1 = freq_1(doc_all)\n",
    "    x = 0\n",
    "    box_all = []\n",
    "    for i in step_1: # 0: pos, 1: neu, 2: neg\n",
    "        box_all.append([])\n",
    "        for j in range(len(i)): # extract keyword: j\n",
    "            box_all[x].append([])\n",
    "            for k in level_all[x]:\n",
    "                if (i[j] in k) and (len(k)>len(i[j])):\n",
    "                    box_all[x][j].append(k)\n",
    "        x += 1\n",
    "    z = 0\n",
    "    rebox_all = []\n",
    "    for i in box_all:\n",
    "        rebox_all.append([])\n",
    "        for j in range(len(i)):\n",
    "            rebox_all[z].append(freq(level_all[z],i[j]))\n",
    "        z += 1\n",
    "    return rebox_all\n",
    "\n",
    "# file 열기\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "curDir = os.getcwd()\n",
    "# path에 파일명 입력\n",
    "# example\n",
    "# path = curDir+'/raw_data/twitter_notadv.txt'\n",
    "\n",
    "######################################\n",
    "############# INPUT HERE #############\n",
    "######################################\n",
    "\n",
    "path = '/Users/EternalFlow/Desktop/indatalab/nlp/projects/wannaone(new)/result_data/twit_170811_wannaone'\n",
    "files = allfiles(path)\n",
    "\n",
    "# 단일 파일의 경우 for 문을 제거하고 path에 절대경로 입력\n",
    "# 마지막에 writer 경로에 새폴더를 넣을 경우 미리 폴더를 만들어주어야 함\n",
    "for i in files:\n",
    "    path = i\n",
    "    original_file = path[path.rfind('/')+1:path.rfind('.')]\n",
    "    #input_data=pd.read_table(path,delimiter='\\t',encoding='utf-8',header=None)\n",
    "    input_data=pd.read_excel(path,encoding='utf-8')\n",
    "    input_data = input_data.dropna(axis=0)\n",
    "    contents = input_data['contents'].as_matrix()\n",
    "\n",
    "    # channel = input_data['channel'].as_matrix()\n",
    "    # date = input_data['date'].as_matrix()\n",
    "    # contents = input_data['contents'].as_matrix()\n",
    "    point = input_data['point'].as_matrix()\n",
    "    # label = input_data['predict_label'].as_matrix()\n",
    "    # sub_contents = ex_keyword(contents,'워너원')\n",
    "    # point: 50 ~ 100\n",
    "    level_a = []\n",
    "    for i in range(len(contents)):\n",
    "        if point[i] >= 50:\n",
    "            level_a.append(str(contents[i]))\n",
    "\n",
    "    # point: -50 ~ 50\n",
    "    level_b = []\n",
    "    for i in range(len(contents)):\n",
    "        if point[i] >= -50 and point[i] < 50 :\n",
    "            level_b.append(str(contents[i]))\n",
    "\n",
    "    # point: -100 ~ -50\n",
    "    level_c = []\n",
    "    for i in range(len(contents)):\n",
    "        if point[i] < -50:\n",
    "            level_c.append(str(contents[i]))\n",
    "    doc_a = ''.join(level_a)\n",
    "    doc_b = ''.join(level_b)\n",
    "    doc_c = ''.join(level_c)\n",
    "    level_all = [level_a,level_b,level_c]\n",
    "    doc_all = [doc_a,doc_b,doc_c]\n",
    "\n",
    "    result_set = freq_1(doc_all)\n",
    "    test = main(doc_all,level_all)\n",
    "    subset_a = pd.DataFrame(data=test[0],index=result_set[0])\n",
    "    subset_b = pd.DataFrame(data=test[1],index=result_set[1])\n",
    "    subset_c = pd.DataFrame(data=test[2],index=result_set[2])\n",
    "    writer = pd.ExcelWriter(curDir+'/word_data/twit_170811_recap/word_'+original_file+'.xlsx') #here\n",
    "    subset_a.to_excel(writer,'Positve')\n",
    "    subset_b.to_excel(writer,'Neutral')\n",
    "    subset_c.to_excel(writer,'Negative')\n",
    "    writer.save()\n",
    "    print(original_file)\n",
    "    '''\n",
    "    except:\n",
    "        print(path)\n",
    "        count_pos = len(input_data.loc[input_data[5] == 'POS'])\n",
    "        count_neu = len(input_data.loc[input_data[5] == 'NEU'])\n",
    "        count_neg = len(input_data.loc[input_data[5] == 'NEG'])\n",
    "        count_all = len(input_data)\n",
    "        counts = [count_pos,count_neu,count_neg]\n",
    "        print(\"긍정: {}\".format(count_pos), \"비중: {:.2f}\".format(count_pos/count_all))\n",
    "        print(\"중립: {}\".format(count_neu), \"비중: {:.2f}\".format(count_neu/count_all))\n",
    "        print(\"부정: {}\".format(count_neg), \"비중: {:.2f}\".format(count_neg/count_all))\n",
    "        print(i)\n",
    "        pass\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
