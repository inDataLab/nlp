{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Model\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만들어진 모델을 불러오는데 필요한 코드\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression as lgt\n",
    "import re\n",
    "\n",
    "# tokeinize - Twitter\n",
    "from konlpy.tag import Twitter\n",
    "twit = Twitter()\n",
    "\n",
    "curDir = os.getcwd() # curDir = 현재 경로 / 폴더명1 / 폴더명 2 / 파일명.pkl\n",
    "\n",
    "# model1\n",
    "words1 = ['Adjective','Adverb','Conjunction','Exclamation','KoreanParticle','PreEomi','Eomi','Hashtag']\n",
    "def tokenize1(doc):\n",
    "    tagged = ['/'.join(t) for t in twit.pos(doc, norm=True, stem=True)]\n",
    "    result = [word for word in tagged if word.split('/')[-1] in words1]\n",
    "    return result\n",
    "model1 = pickle.load(open(os.path.join(curDir,'model1.pkl'),'rb')) # call model\n",
    "tfidf1 = pickle.load(open(os.path.join(curDir,'tfidf1.pkl'),'rb')) # call vectorizer\n",
    "\n",
    "# model2\n",
    "words2 = ['Noun','Josa','Adjective','Adverb','Exclamation','KoreanParticle','PreEomi','Eomi']\n",
    "def tokenize2(doc):\n",
    "    tagged = ['/'.join(t) for t in twit.pos(doc, norm=True, stem=True)]\n",
    "    result = [word for word in tagged if word.split('/')[-1] in words2]\n",
    "    return result\n",
    "model2 = pickle.load(open(os.path.join(curDir,'model2.pkl'),'rb')) # call model\n",
    "tfidf2 = pickle.load(open(os.path.join(curDir,'tfidf2.pkl'),'rb')) # call vectorizer\n",
    "\n",
    "# model3\n",
    "def tokenize3(doc):\n",
    "    tagged = ['/'.join(t) for t in twit.pos(doc, norm=True, stem=True)]\n",
    "    result = [word for word in tagged if word.split('/')[-1]]\n",
    "    return result\n",
    "\n",
    "model3 = pickle.load(open(os.path.join(curDir,'model3.pkl'),'rb')) # call model\n",
    "tfidf3 = pickle.load(open(os.path.join(curDir,'tfidf3.pkl'),'rb')) # call vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data_Set\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "curDir = os.getcwd()\n",
    "# path에 파일명 입력\n",
    "path = curDir+'/raw_data/101_twit.xlsx'\n",
    "original_file = path[path.rfind('/'):]\n",
    "df_test=pd.read_excel(path,header=None)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "curDir = os.getcwd()\n",
    "# path에 파일명 입력\n",
    "path = curDir+'/raw_data/101_twit.txt'\n",
    "original_file = path[path.rfind('/')+1:path.rfind('.')]\n",
    "df_test=pd.read_table(path,delimiter='\\t')\n",
    "h_x = df_test['contents'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(model,tfidf,input_data):\n",
    "    target = tfidf.transform(h_x)\n",
    "    result = model.predict(target)\n",
    "    result_p = model.predict_proba(target)\n",
    "    df_prob = pd.DataFrame(result_p)\n",
    "    result_p0 = df_prob[0]\n",
    "    result_p1 = df_prob[1]\n",
    "    point = round((result_p1 - result_p0),2)*100\n",
    "    return point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = score(model1,tfidf1,h_x) # n_gram1\n",
    "score2 = score(model2,tfidf2,h_x) # n_gram2\n",
    "score3 = score(model3,tfidf3,h_x) # n_gram3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = score1/2 + score2/4 + score3/4\n",
    "label = []\n",
    "for i in  range(len(result)):\n",
    "    if result[i] >= 50:\n",
    "        label.append('POS')\n",
    "    elif result[i] > -50:\n",
    "        label.append('NEU')\n",
    "    else:\n",
    "        label.append('NEG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Data\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {'point':result,'predict_label':label}\n",
    "labeled = pd.DataFrame(data)\n",
    "output = df_test.join(labeled,lsuffix='_df_test', rsuffix='labeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/EternalFlow/anaconda/lib/python3.6/site-packages/xlsxwriter/worksheet.py:830: UserWarning: Ignoring URL 'https://form.office.naver.com/form/responseViewMobile.cmd?formkey=ZWQ1YzM1ZTAtODA1Mi00ZDM5LWI2YTctNjU5MDU4MDMwOTI2&sourceId=urlshare%20…%20%3c워너원%20보틀%20&%20떡메%20주문%20받습니다%3e%20폼에서%20설명%20찬찬히%20잘%20읽어보시고%20신중한%20주문부탁드립니다!%20-리트윗%20하신%20분들%20중%20추첨을%20통해%203분께%20무료나눔%20하고자%20합니다!!!%20' with link or location/anchor > 255 characters since it exceeds Excel's limit for URLS\n",
      "  force_unicode(url))\n"
     ]
    }
   ],
   "source": [
    "output.to_excel(curDir+'/result_data/predicted_'+original_file+'.xlsx',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
