{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# wannaone\n",
    "path = r'predicted_101_twit.xlsx'\n",
    "input_data=pd.read_excel(r'predicted_101_twit.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "channel = input_data['channel'].as_matrix()\n",
    "## date = input_data['date'].as_matrix()\n",
    "contents = input_data['contents'].as_matrix()\n",
    "point = input_data['point'].as_matrix()\n",
    "label = input_data['predict_label'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_pos = len(input_data.loc[input_data['predict_label'] == 'POS'])\n",
    "count_neu = len(input_data.loc[input_data['predict_label'] == 'NEU'])\n",
    "count_neg = len(input_data.loc[input_data['predict_label'] == 'NEG'])\n",
    "counts = [count_pos,count_neu,count_neg]\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing Data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def ex_keyword(h_x,keyword):\n",
    "    for i in range(len(h_x)):\n",
    "        try:\n",
    "            h_x[i] = h_x[i].replace(keyword,\"\")\n",
    "        except:\n",
    "            h_x[i] = str(h_x[i])\n",
    "            pass\n",
    "    return h_x\n",
    "\n",
    "sub_contents = ex_keyword(contents,'워너원')\n",
    "sub_contents = ex_keyword(sub_contents,'WANNAONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokeinize - Twitter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Twitter\n",
    "stopwords = ['Josa','PreEomi','Punctuation','Foreign','Alpha','Unknown','Eomi']\n",
    "\n",
    "def tokenizer2 (doc):\n",
    "    tagged = ['/'.join(t) for t in twit.pos(doc, norm=True, stem=True)]\n",
    "    result_words = [word for word in tagged if word.split('/')[-1] == 'Noun' or 'Verb']\n",
    "    result_nouns = [word.split('/')[0] for word in result_words if len(word.split('/')[0]) > 1]\n",
    "    return result_nouns\n",
    " \n",
    "vect2 = CountVectorizer(tokenizer=tokenizer2,ngram_range=(1,3),min_df=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Pos / Neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# point: 50 ~ 100\n",
    "level_a = []\n",
    "for i in range(len(sub_contents)):\n",
    "    if point[i] >= 50:\n",
    "        level_a.append(str(sub_contents[i]))\n",
    "\n",
    "# point: -50 ~ 50\n",
    "level_b = []\n",
    "for i in range(len(sub_contents)):\n",
    "    if point[i] >= -50 and point[i] < 50 :\n",
    "        level_b.append(str(sub_contents[i]))\n",
    "\n",
    "# point: -100 ~ -50\n",
    "level_c = []\n",
    "for i in range(len(sub_contents)):\n",
    "    if point[i] < -50:\n",
    "        level_c.append(str(sub_contents[i]))\n",
    "doc_a = ''.join(level_a)\n",
    "doc_b = ''.join(level_b)\n",
    "doc_c = ''.join(level_c)\n",
    "doc_all = [doc_a,doc_b,doc_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Twitter\n",
    "twit = Twitter()\n",
    "def tokenizer2 (doc):\n",
    "    tagged = ['/'.join(t) for t in twit.pos(doc, norm=True, stem=True)]\n",
    "    result_words = [word for word in tagged if word.split('/')[-1] == 'Noun']\n",
    "    result_nouns = [word.split('/')[0] for word in result_words if len(word.split('/')[0]) > 1]\n",
    "    return result_nouns\n",
    "\n",
    "count_vect = CountVectorizer(tokenizer=tokenizer2, min_df=3)\n",
    "count_all = count_vect.fit_transform(doc_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_a = count_vect.transform(level_a)\n",
    "count_b = count_vect.transform(level_b)\n",
    "count_c = count_vect.transform(level_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer().fit(count_all)\n",
    "tf_all = tf_transformer.transform(count_all)\n",
    "tf_a = tf_transformer.transform(count_a)\n",
    "tf_b = tf_transformer.transform(count_b)\n",
    "tf_c = tf_transformer.transform(count_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_all_array = tf_all.toarray()\n",
    "sort_index = tf_all_array.argsort(axis=1)\n",
    "count_dict = count_vect.vocabulary_\n",
    "\n",
    "for index in sort_index[0][-20:]:\n",
    "    important_word = index\n",
    "    key = list(count_dict.keys())[list(count_dict.values()).index(important_word)]\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_dict = count_vect.vocabulary_\n",
    "for index in sort_index[1][-20:]:\n",
    "    important_word = index\n",
    "    key = list(count_dict.keys())[list(count_dict.values()).index(important_word)]\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_dict = count_vect.vocabulary_\n",
    "for index in sort_index[2][-20:]:\n",
    "    important_word = index\n",
    "    key = list(count_dict.keys())[list(count_dict.values()).index(important_word)]\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topics(topics, feature_names, sorting, topics_per_chunk=6, n_words=20):\n",
    "    topic_sets = []\n",
    "    for j in range(len(topics)):\n",
    "        topic_sets.append([])\n",
    "    # print top n_words frequent words\n",
    "        for i in range(n_words):\n",
    "            try:\n",
    "                topic_sets[j].append(feature_names[sorting[j, i]])\n",
    "            except:\n",
    "                pass\n",
    "    return topic_sets \n",
    "\n",
    "def doc_topics(topic_sets):\n",
    "    doc_topics = []\n",
    "    for i in range(len(topic_sets)):\n",
    "        temp = ' '.join(topic_sets[i])\n",
    "        doc_topics.append(temp)\n",
    "    return doc_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from math import log\n",
    "\n",
    "for i in range(1,10):\n",
    "    lda = LatentDirichletAllocation(n_topics=i, learning_method=\"batch\", max_iter=10, random_state=0)\n",
    "    document_topics = lda.fit_transform(tf_a)\n",
    "    BIC = -tf_a.shape[1]*log(lda.perplexity(tf_a))-(i*(tf_a.shape[1]+1)/2)*log(tf_a.shape[0])\n",
    "    print(i, lda.perplexity(tf_a), BIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recommend you to use the number of topics which has minimum value of perplexity\n",
    "\n",
    "lda_p = LatentDirichletAllocation(n_topics=2, learning_method=\"batch\", max_iter=20, random_state=0)\n",
    "document_topics_p = lda_p.fit_transform(tf_a)\n",
    "\n",
    "import numpy as np\n",
    "sorting = np.argsort(lda_p.components_, axis=1)[:,::-1]\n",
    "feature_names = np.array(count_vect.get_feature_names())\n",
    "\n",
    "import mglearn\n",
    "mglearn.tools.print_topics(topics=range(2), feature_names=feature_names, sorting=sorting, \n",
    "                           topics_per_chunk=5, n_words=20)\n",
    "\n",
    "#pos_topic = get_topics(topics=range(5), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=20)\n",
    "#pos_topics = doc_topics(pos_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import mglearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "color_pos = np.array(Image.open(\"blue.jpg\"))\n",
    "image_colors = ImageColorGenerator(color_pos)\n",
    "\n",
    "wordcloud = WordCloud(font_path =r\"C:\\Windows\\Fonts\\NanumGothic.ttf\",\n",
    "                     relative_scaling = 0.2, background_color = 'white',\n",
    "                      mask = color_pos,\n",
    "                     min_font_size=1,max_font_size=40).generate(pos_topics[0])\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.imshow(wordcloud.recolor(color_func=image_colors),interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('pos_topic.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "for i in range(1,5):\n",
    "    lda = LatentDirichletAllocation(n_topics=i, learning_method=\"batch\", max_iter=10, random_state=0)\n",
    "    document_topics = lda.fit_transform(tf_b)\n",
    "    print(i, lda.perplexity(tf_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recommend you to use the number of topics which has minimum value of perplexity\n",
    "\n",
    "lda_p = LatentDirichletAllocation(n_topics=5, learning_method=\"batch\", max_iter=20, random_state=0)\n",
    "document_topics_p = lda_p.fit_transform(tf_b)\n",
    "\n",
    "import numpy as np\n",
    "sorting = np.argsort(lda_p.components_, axis=1)[:,::-1]\n",
    "feature_names = np.array(count_vect.get_feature_names())\n",
    "\n",
    "import mglearn\n",
    "mglearn.tools.print_topics(topics=range(5), feature_names=feature_names, sorting=sorting, \n",
    "                           topics_per_chunk=5, n_words=20)\n",
    "\n",
    "#neu_topic = get_topics(topics=range(5), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=20)\n",
    "#neu_topics = doc_topics(neu_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import mglearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "color_neu = np.array(Image.open(\"yellow.jpg\"))\n",
    "image_colors = ImageColorGenerator(color_neu)\n",
    "\n",
    "wordcloud = WordCloud(font_path =r\"C:\\Windows\\Fonts\\NanumGothic.ttf\",\n",
    "                     relative_scaling = 0.2, background_color = 'white',\n",
    "                      mask = color_neu,\n",
    "                     min_font_size=1,max_font_size=40).generate(neu_topics[0])\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.imshow(wordcloud.recolor(color_func=image_colors),interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('neu_topic.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recommend you to use the number of topics which has minimum value of perplexity\n",
    "\n",
    "lda_p = LatentDirichletAllocation(n_topics=5, learning_method=\"batch\", max_iter=20, random_state=0)\n",
    "document_topics_p = lda_p.fit_transform(tf_c)\n",
    "\n",
    "import numpy as np\n",
    "sorting = np.argsort(lda_p.components_, axis=1)[:,::-1]\n",
    "feature_names = np.array(count_vect.get_feature_names())\n",
    "\n",
    "import mglearn\n",
    "mglearn.tools.print_topics(topics=range(5), feature_names=feature_names, sorting=sorting, \n",
    "                           topics_per_chunk=5, n_words=20)\n",
    "\n",
    "neg_topic = get_topics(topics=range(5), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=20)\n",
    "neg_topics = doc_topics(neg_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import mglearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "color_neg = np.array(Image.open(\"red.jpg\"))\n",
    "image_colors = ImageColorGenerator(color_neg)\n",
    "\n",
    "wordcloud = WordCloud(font_path =r\"C:\\Windows\\Fonts\\NanumGothic.ttf\",\n",
    "                     relative_scaling = 0.2, background_color = 'white',\n",
    "                      mask = color_neg,\n",
    "                     min_font_size=1,max_font_size=40).generate(neg_topics[0])\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.imshow(wordcloud.recolor(color_func=image_colors),interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('neg_topic.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
