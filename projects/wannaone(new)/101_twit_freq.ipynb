{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# wannaone\n",
    "input_data=pd.read_excel(r'predicted_101_twit.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "channel = input_data['channel'].as_matrix()\n",
    "## date = input_data['date'].as_matrix()\n",
    "contents = input_data['contents'].as_matrix()\n",
    "point = input_data['point'].as_matrix()\n",
    "label = input_data['predict_label'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_pos = len(input_data.loc[input_data['predict_label'] == 'POS'])\n",
    "count_neu = len(input_data.loc[input_data['predict_label'] == 'NEU'])\n",
    "count_neg = len(input_data.loc[input_data['predict_label'] == 'NEG'])\n",
    "count_all = len(input_data)\n",
    "counts = [count_pos,count_neu,count_neg]\n",
    "print(\"긍정: {}\".format(count_pos), \"비중: {:.2f}\".format(count_pos/count_all))\n",
    "print(\"중립: {}\".format(count_neu), \"비중: {:.2f}\".format(count_neu/count_all))\n",
    "print(\"부정: {}\".format(count_neg), \"비중: {:.2f}\".format(count_neg/count_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing Data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def ex_keyword(h_x,keyword):\n",
    "    temp = h_x\n",
    "    for i in range(len(h_x)):\n",
    "        try:\n",
    "            temp[i] = temp[i].replace(keyword,\"\")\n",
    "        except:\n",
    "            temp[i] = str(temp[i])\n",
    "            pass\n",
    "    return temp\n",
    "\n",
    "sub_contents = ex_keyword(contents,'워너원')\n",
    "sub_contents = ex_keyword(sub_contents,'WANNAONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Hannanum\n",
    "han = Hannanum()\n",
    "twit = Twitter()\n",
    "\n",
    "def tokenizer (doc):\n",
    "    tagged = ['/'.join(t) for t in han.pos(doc)]\n",
    "    result_words = [word for word in tagged if word.split('/')[-1] == 'N']\n",
    "    result_nouns = [word.split('/')[0] for word in result_words if len(word.split('/')[0]) > 1]\n",
    "    step_2 = ' '.join(result_nouns)\n",
    "    tagged = ['/'.join(t) for t in twit.pos(step_2)]\n",
    "    result_words = [word for word in tagged if word.split('/')[-1] == 'Noun']\n",
    "    result_nouns = [word.split('/')[0] for word in result_words if len(word.split('/')[0]) > 1]\n",
    "    step_3 = ' '.join(result_nouns)\n",
    "    return step_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# point: 50 ~ 100\n",
    "\n",
    "level_a = []\n",
    "for i in range(len(sub_contents)):\n",
    "    if point[i] >= 50:\n",
    "        level_a.append(tokenizer(sub_contents[i]))\n",
    "\n",
    "# point: -50 ~ 50\n",
    "level_b = []\n",
    "for i in range(len(sub_contents)):\n",
    "    if point[i] >= -50 and point[i] < 50 :\n",
    "        level_b.append(tokenizer(sub_contents[i]))\n",
    "\n",
    "# point: -100 ~ -50\n",
    "level_c = []\n",
    "for i in range(len(sub_contents)):\n",
    "    if point[i] < -50:\n",
    "        level_c.append(tokenizer(sub_contents[i]))\n",
    "doc_a = ''.join(level_a)\n",
    "doc_b = ''.join(level_b)\n",
    "doc_c = ''.join(level_c)\n",
    "doc_all = [doc_a,doc_b,doc_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# freq_1: 첫번째\n",
    "def freq_1(level_b):\n",
    "    doc_b = ''.join(level_b)\n",
    "    frequency = {}\n",
    "    text_string = doc_b\n",
    "    match_pattern = re.findall(r'\\b[가-힣]{2,10}\\b', text_string)\n",
    "    for word in match_pattern:\n",
    "        count = frequency.get(word,0)\n",
    "        frequency[word] = count + 1\n",
    "    frequency_list = frequency.keys()\n",
    "    sorted_freq_b = sorted(frequency.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    subset_b = []\n",
    "    for i in range(0,20):\n",
    "        subset_b.append(sorted_freq_b[i][0])\n",
    "    return subset_b\n",
    "\n",
    "# freq: 두번째\n",
    "def freq_2(level_b):\n",
    "    doc_b = ''.join(level_b)\n",
    "    frequency = {}\n",
    "    text_string = doc_b\n",
    "    match_pattern = re.findall(r'\\b[가-힣]{2,10}\\b', text_string)\n",
    "    for word in match_pattern:\n",
    "        count = frequency.get(word,0)\n",
    "        frequency[word] = count + 1\n",
    "    frequency_list = frequency.keys()\n",
    "    sorted_freq_b = sorted(frequency.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    subset_b = []\n",
    "    for i in range(1,6):\n",
    "        subset_b.append(sorted_freq_b[i][0])\n",
    "    return subset_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(level_b):\n",
    "    step_1 = freq_1(level_b)\n",
    "    x=0\n",
    "    box_b = []\n",
    "    for i in step_1:\n",
    "        box_b.append([])\n",
    "        for j in level_b:\n",
    "            if (i in j) and (len(j)>len(i)):\n",
    "                box_b[x].append(j)\n",
    "        x +=1\n",
    "    rebox_b = []\n",
    "    for i in range(len(box_b)):\n",
    "        rebox_b.append(freq(box_b[i]))\n",
    "    result = pd.DataFrame(data=rebox_b,index=step_1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEVEL_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "frequency = {}\n",
    "text_string = doc_a\n",
    "match_pattern = re.findall(r'\\b[가-힣]{2,10}\\b', text_string)\n",
    "for word in match_pattern:\n",
    "    count = frequency.get(word,0)\n",
    "    frequency[word] = count + 1\n",
    "     \n",
    "frequency_list = frequency.keys()\n",
    "\n",
    "import operator \n",
    "sorted_freq_a = sorted(frequency.items(), key=operator.itemgetter(1),reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_freq_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEVEL_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "frequency = {}\n",
    "text_string = doc_b\n",
    "match_pattern = re.findall(r'\\b[가-힣]{2,10}\\b', text_string)\n",
    "for word in match_pattern:\n",
    "    count = frequency.get(word,0)\n",
    "    frequency[word] = count + 1\n",
    "     \n",
    "frequency_list = frequency.keys()\n",
    "\n",
    "import operator \n",
    "sorted_freq_b = sorted(frequency.items(), key=operator.itemgetter(1),reverse=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEVEL_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "frequency = {}\n",
    "text_string = doc_c\n",
    "match_pattern = re.findall(r'\\b[가-힣]{2,10}\\b', text_string)\n",
    "for word in match_pattern:\n",
    "    count = frequency.get(word,0)\n",
    "    frequency[word] = count + 1\n",
    "     \n",
    "frequency_list = frequency.keys()\n",
    "\n",
    "import operator \n",
    "sorted_freq_c = sorted(frequency.items(), key=operator.itemgetter(1),reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main(level_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main(level_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main(level_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
